{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EvoForge import EvoForge  \n",
    "evoforge_client = EvoForge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yunhengzou@mariana:~/genaihackthon$ PROMPT$ bash: cd: /h/399/yunhengzou/genaihackthon/temporary_work_dir/EvoForge1: No such file or directory\n",
      "PROMPT$ Deleted 11 documents from 'checkpoints' collection for thread_id 'EvoForge1'.\n",
      "Deleted 34 documents from 'checkpoint_writes' collection for thread_id 'EvoForge1'.\n"
     ]
    }
   ],
   "source": [
    "agent = evoforge_client.spawn_setup_agent(session=\"EvoForge1\")\n",
    "agent.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: execute_shell_command with args: {'shell_command': 'micromamba activate hf'}\n",
      "response\n",
      "content=[{'text': \"I'll help you set up and test the nilq/mistral-1L-tiny model from Hugging Face. Let's start by activating the Hugging Face environment and exploring the model.\", 'type': 'text'}, {'id': 'toolu_01BYFq4vWguhypGddXoG3HdX', 'input': {'shell_command': 'micromamba activate hf'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01SmX8WibTDMsfk9Lzu3U9JP', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 904, 'output_tokens': 106}} id='run-30f9409f-ae97-4267-aa5a-f3bdab250860-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'micromamba activate hf'}, 'id': 'toolu_01BYFq4vWguhypGddXoG3HdX', 'type': 'tool_call'}] usage_metadata={'input_tokens': 904, 'output_tokens': 106, 'total_tokens': 1010, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "(hf) PROMPT$ The current progress is  10\n",
      "Tool call: read_webpage with args: {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}\n",
      "response\n",
      "content=[{'text': \"Now, let's read the model information from Hugging Face to understand what we're working with:\", 'type': 'text'}, {'id': 'toolu_01SJvdoBjWncjM9zFRxnRkJY', 'input': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'name': 'read_webpage', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01UAQiSjw181h4RYwuqrShmR', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1023, 'output_tokens': 94}} id='run-accf84af-d0f8-4aae-9939-89a1f085e6ad-0' tool_calls=[{'name': 'read_webpage', 'args': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'id': 'toolu_01SJvdoBjWncjM9zFRxnRkJY', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1023, 'output_tokens': 94, 'total_tokens': 1117, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  20\n",
      "Tool call: create_file with args: {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Prepare a prompt\\nprompt = \"Once upon a time, there was a little girl who\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    top_p=0.9,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}\n",
      "response\n",
      "content=[{'text': \"Now let's create a simple test script to use the mistral-1L-tiny model. This model is a small language model trained on the TinyStories dataset. Let's create a Python script to load and test it:\", 'type': 'text'}, {'id': 'toolu_01Ma9tZs5hNVva992rgZYQwA', 'input': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Prepare a prompt\\nprompt = \"Once upon a time, there was a little girl who\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    top_p=0.9,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'name': 'create_file', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01LfrujXukQPVT5qZkxQidZr', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3388, 'output_tokens': 346}} id='run-d564d7e1-92ab-48c1-a3b9-a0225cd22dd6-0' tool_calls=[{'name': 'create_file', 'args': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Prepare a prompt\\nprompt = \"Once upon a time, there was a little girl who\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    top_p=0.9,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'id': 'toolu_01Ma9tZs5hNVva992rgZYQwA', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3388, 'output_tokens': 346, 'total_tokens': 3734, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  50\n",
      "Tool call: execute_shell_command with args: {'shell_command': 'python test_mistral_tiny.py'}\n",
      "response\n",
      "content=[{'text': \"Now let's run the test script to see if it works:\", 'type': 'text'}, {'id': 'toolu_01MkvPLdxNTcteaUtrdsP1xD', 'input': {'shell_command': 'python test_mistral_tiny.py'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01MnfTfhsJMUNbwXcuNvamTN', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3753, 'output_tokens': 80}} id='run-a6536772-586d-47d4-80bf-1c4b74fc62a0-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'python test_mistral_tiny.py'}, 'id': 'toolu_01MkvPLdxNTcteaUtrdsP1xD', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3753, 'output_tokens': 80, 'total_tokens': 3833, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "Traceback (most recent call last):\n",
      "  File \"/h/399/yunhengzou/genaihackthon/test_mistral_tiny.py\", line 1, in <module>\n",
      "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "ModuleNotFoundError: No module named 'transformers'\n",
      "(hf) PROMPT$ The current progress is  50\n"
     ]
    }
   ],
   "source": [
    "result = agent.stream_return_graph_state(\"https://huggingface.co/nilq/mistral-1L-tiny, provide me a test example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from EvoForge import EvoForge  \n",
    "evoforge_client = EvoForge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yunhengzou@mariana:~/genaihackthon$ PROMPT$ PROMPT$ Deleted 59 documents from 'checkpoints' collection for thread_id 'EvoForge1'.\n",
      "Deleted 199 documents from 'checkpoint_writes' collection for thread_id 'EvoForge1'.\n"
     ]
    }
   ],
   "source": [
    "agent = evoforge_client.spawn_setup_agent(session=\"EvoForge1\")\n",
    "agent.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: execute_shell_command with args: {'shell_command': 'micromamba activate hf'}\n",
      "response\n",
      "content=[{'text': \"I'll help you set up and test the nilq/mistral-1L-tiny model from Hugging Face. Let's start by activating the Hugging Face environment and then create a simple test example.\", 'type': 'text'}, {'id': 'toolu_01JAtHfNDNdzaMLT8ujWFuoW', 'input': {'shell_command': 'micromamba activate hf'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01TWmttvpM93Zbv7emDAVbrw', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 933, 'output_tokens': 109}} id='run-7dd2bd63-9845-4692-a86a-0f5580c8d7fe-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'micromamba activate hf'}, 'id': 'toolu_01JAtHfNDNdzaMLT8ujWFuoW', 'type': 'tool_call'}] usage_metadata={'input_tokens': 933, 'output_tokens': 109, 'total_tokens': 1042, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "(hf) PROMPT$ The current progress is  10\n",
      "Tool call: read_webpage with args: {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}\n",
      "response\n",
      "content=[{'text': \"Now, let's check the Hugging Face model page to understand what this model is and how to use it:\", 'type': 'text'}, {'id': 'toolu_01Ecr5YwsyZq4cwSpRRoV63o', 'input': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'name': 'read_webpage', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01A2mHsYjmqYGhrQKae1QmHf', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1087, 'output_tokens': 97}} id='run-78cc3a3a-1740-46d0-a8e7-7cef5b8c167d-0' tool_calls=[{'name': 'read_webpage', 'args': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'id': 'toolu_01Ecr5YwsyZq4cwSpRRoV63o', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1087, 'output_tokens': 97, 'total_tokens': 1184, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  20\n",
      "Tool call: create_file with args: {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Define a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n# Print the result\\nprint(\"Generated text:\")\\nprint(generated_text)'}\n",
      "response\n",
      "content=[{'text': \"Now I'll create a simple test example to use the nilq/mistral-1L-tiny model. This is a small Mistral model trained on the TinyStories dataset for text generation.\", 'type': 'text'}, {'id': 'toolu_01RpZ5ty5ADdUU9St2Zgoxhz', 'input': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Define a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n# Print the result\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'name': 'create_file', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01Y9thVnss4ZK1tZtx9fUcHV', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3485, 'output_tokens': 333}} id='run-6fa05bd6-9705-4e8f-a817-645e00b2160c-0' tool_calls=[{'name': 'create_file', 'args': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Define a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n# Print the result\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'id': 'toolu_01RpZ5ty5ADdUU9St2Zgoxhz', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3485, 'output_tokens': 333, 'total_tokens': 3818, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  60\n",
      "Tool call: execute_shell_command with args: {'shell_command': 'python test_mistral_tiny.py'}\n",
      "response\n",
      "content=[{'text': \"Now let's run the test example to see if it works:\", 'type': 'text'}, {'id': 'toolu_01PzbUgLRcvxwqL18R76XDJE', 'input': {'shell_command': 'python test_mistral_tiny.py'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_019GSEGVe1V41xhzFTNhYqgL', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3878, 'output_tokens': 80}} id='run-dd4d14b8-5746-4c33-9bd4-0fd78a16013c-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'python test_mistral_tiny.py'}, 'id': 'toolu_01PzbUgLRcvxwqL18R76XDJE', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3878, 'output_tokens': 80, 'total_tokens': 3958, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n"
     ]
    }
   ],
   "source": [
    "result = agent.stream_return_graph_state(\"https://huggingface.co/nilq/mistral-1L-tiny, provide me a test example, one minimum example is enough, do not need other tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: read_webpage with args: {'url': 'https://unsplash.com/photos/russian-blue-cat-wearing-yellow-sunglasses-yMSecCHsIBc'}\n",
      "response\n",
      "content=[{'text': \"I'll download the photo of the Russian blue cat wearing yellow sunglasses from Unsplash. Let me do that for you.\", 'type': 'text'}, {'id': 'toolu_01RcjiNBKWM8jz6HuweFrnRU', 'input': {'url': 'https://unsplash.com/photos/russian-blue-cat-wearing-yellow-sunglasses-yMSecCHsIBc'}, 'name': 'read_webpage', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_014woMGc47t57o95EUZKeSxa', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 7080, 'output_tokens': 114}} id='run-074ef402-9ba5-4c07-9ca7-1176d4dd60d0-0' tool_calls=[{'name': 'read_webpage', 'args': {'url': 'https://unsplash.com/photos/russian-blue-cat-wearing-yellow-sunglasses-yMSecCHsIBc'}, 'id': 'toolu_01RcjiNBKWM8jz6HuweFrnRU', 'type': 'tool_call'}] usage_metadata={'input_tokens': 7080, 'output_tokens': 114, 'total_tokens': 7194, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  0\n",
      "Tool call: execute_shell_command with args: {'shell_command': 'curl -L \"https://images.unsplash.com/photo-1533738363-b7f9aef128ce?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" -o cat_with_sunglasses.jpg'}\n",
      "response\n",
      "content=[{'text': \"Now I'll download the image from the direct download link I found on the page:\", 'type': 'text'}, {'id': 'toolu_01V12pP251GupSzWFD79o6wR', 'input': {'shell_command': 'curl -L \"https://images.unsplash.com/photo-1533738363-b7f9aef128ce?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" -o cat_with_sunglasses.jpg'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_0196VAJY6CZEJM19sACZADkc', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 8613, 'output_tokens': 190}} id='run-c0a08ee8-4fd9-4d8d-add8-ab65aa7755ff-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'curl -L \"https://images.unsplash.com/photo-1533738363-b7f9aef128ce?fm=jpg&q=60&w=3000&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" -o cat_with_sunglasses.jpg'}, 'id': 'toolu_01V12pP251GupSzWFD79o6wR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 8613, 'output_tokens': 190, 'total_tokens': 8803, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2259k  100 2259k    0     0  31.0M      0 --:--:-- --:--:-- --:--:-- 31.0M\n",
      "(hf) PROMPT$ The current progress is  100\n",
      "response\n",
      "Great! I've successfully downloaded the image of the Russian blue cat wearing yellow sunglasses from Unsplash. The image has been saved as \"cat_with_sunglasses.jpg\" in your current directory.\n",
      "\n",
      "The download is complete and the file is now available in your working directory. The image is approximately 2.2 MB in size.\n",
      "EvoForge1\n",
      "default_name\n"
     ]
    }
   ],
   "source": [
    "result = agent.stream_return_graph_state(\"downlaod this photo https://unsplash.com/photos/russian-blue-cat-wearing-yellow-sunglasses-yMSecCHsIBc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

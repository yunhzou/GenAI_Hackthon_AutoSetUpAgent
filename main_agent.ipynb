{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from EvoForge import EvoForge  \n",
    "evoforge_client = EvoForge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yunhengzou@mariana:~/genaihackthon$ PROMPT$ PROMPT$ Deleted 35 documents from 'checkpoints' collection for thread_id 'EvoForge1'.\n",
      "Deleted 112 documents from 'checkpoint_writes' collection for thread_id 'EvoForge1'.\n"
     ]
    }
   ],
   "source": [
    "agent = evoforge_client.spawn_setup_agent(session=\"EvoForge1\")\n",
    "agent.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: read_webpage with args: {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}\n",
      "response\n",
      "content=[{'text': \"I'll help you set up and test the nilq/mistral-1L-tiny model from Hugging Face. Let me first check the model page to understand what it is and how to use it.\", 'type': 'text'}, {'id': 'toolu_013nQSou7KSNrbVfPiFFxN1R', 'input': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'name': 'read_webpage', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01UACXB4saCefFmYJ3wP4ArU', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 929, 'output_tokens': 116}} id='run-a3dd5905-7ada-40fa-9cd0-3f7d7447fc98-0' tool_calls=[{'name': 'read_webpage', 'args': {'url': 'https://huggingface.co/nilq/mistral-1L-tiny'}, 'id': 'toolu_013nQSou7KSNrbVfPiFFxN1R', 'type': 'tool_call'}] usage_metadata={'input_tokens': 929, 'output_tokens': 116, 'total_tokens': 1045, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  10\n",
      "Tool call: execute_shell_command with args: {'shell_command': 'micromamba activate hf'}\n",
      "response\n",
      "content=[{'text': \"Now I'll activate the Hugging Face environment and create a simple test script to use the nilq/mistral-1L-tiny model. This is a small Mistral model with a single layer, trained on the TinyStories dataset for generating simple English text.\", 'type': 'text'}, {'id': 'toolu_01XqLcAYz6yFCqSxFVCicZoR', 'input': {'shell_command': 'micromamba activate hf'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01RF5ruqsFNyH9Y3UWsbFXqp', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3316, 'output_tokens': 122}} id='run-9908cd53-7eb7-4563-a881-dd57ef271cf8-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'micromamba activate hf'}, 'id': 'toolu_01XqLcAYz6yFCqSxFVCicZoR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3316, 'output_tokens': 122, 'total_tokens': 3438, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "(hf) PROMPT$ The current progress is  20\n",
      "Tool call: create_file with args: {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Set a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n    top_p=0.95,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}\n",
      "response\n",
      "content=[{'text': \"Now let's create a simple Python script to test the model:\", 'type': 'text'}, {'id': 'toolu_01S2Kvk2ADgy9Yjh5RQVZxaP', 'input': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Set a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n    top_p=0.95,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'name': 'create_file', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01F5oXULBpEYcxmAKSc3d5FH', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3451, 'output_tokens': 308}} id='run-23a38953-03ba-440c-a31a-4e65aa2a941a-0' tool_calls=[{'name': 'create_file', 'args': {'filename': 'test_mistral_tiny.py', 'content': 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Set a prompt\\nprompt = \"Once upon a time, there was a little\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\noutputs = model.generate(\\n    inputs[\"input_ids\"],\\n    max_length=100,\\n    temperature=0.7,\\n    do_sample=True,\\n    top_p=0.95,\\n)\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Generated text:\")\\nprint(generated_text)'}, 'id': 'toolu_01S2Kvk2ADgy9Yjh5RQVZxaP', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3451, 'output_tokens': 308, 'total_tokens': 3759, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The current progress is  50\n",
      "Tool call: execute_shell_command with args: {'shell_command': 'python test_mistral_tiny.py'}\n",
      "response\n",
      "content=[{'text': \"Now let's run the script to test the model:\", 'type': 'text'}, {'id': 'toolu_01ALNLwcLDk4s5Wi6zdVUUPs', 'input': {'shell_command': 'python test_mistral_tiny.py'}, 'name': 'execute_shell_command', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01F1ppf7crYfxjWWkH3URiNB', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 3778, 'output_tokens': 78}} id='run-36d778c6-f0b2-4d15-a7c6-f9c31753b0a3-0' tool_calls=[{'name': 'execute_shell_command', 'args': {'shell_command': 'python test_mistral_tiny.py'}, 'id': 'toolu_01ALNLwcLDk4s5Wi6zdVUUPs', 'type': 'tool_call'}] usage_metadata={'input_tokens': 3778, 'output_tokens': 78, 'total_tokens': 3856, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n",
      "EvoForge1\n",
      "default_name\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy had a toy car that he loved to play with. One day, Timmy's mom asked him to help her with the dishes. Timmy was happy to help and he started to clean up the kitchen.\n",
      "\n",
      "Suddenly, Timmy's mom came in and asked him to help her get it fixed. Timmy was happy to help and he started to clean the dishes. He put them\n",
      "(hf) PROMPT$ The current progress is  100\n",
      "response\n",
      "Great! The model is working correctly. It successfully generated a simple story continuation from the prompt \"Once upon a time, there was a little\".\n",
      "\n",
      "## Summary Report\n",
      "\n",
      "I've successfully set up and tested the nilq/mistral-1L-tiny model from Hugging Face. Here's what was done:\n",
      "\n",
      "1. Activated the pre-installed Hugging Face environment (`micromamba activate hf`)\n",
      "2. Created a test script that:\n",
      "   - Loads the model and tokenizer\n",
      "   - Sets a prompt\n",
      "   - Generates text based on the prompt\n",
      "   - Displays the generated text\n",
      "\n",
      "The model is a tiny version of Mistral with only one layer (35.1M parameters), trained on the TinyStories dataset. It's designed for generating simple English text and is useful for studying feature dynamics in language models.\n",
      "\n",
      "There are some warnings about attention masks and pad tokens, but they don't affect the functionality of the model for this simple test.\n",
      "\n",
      "To use this model in your projects:\n",
      "1. Make sure to activate the Hugging Face environment: `micromamba activate hf`\n",
      "2. Import the necessary libraries from transformers\n",
      "3. Load the model with `AutoModelForCausalLM.from_pretrained(\"nilq/mistral-1L-tiny\")`\n",
      "4. Use the model to generate text based on your prompts\n",
      "\n",
      "The model is lightweight and suitable for educational purposes or for studying how language models work at a basic level.\n",
      "EvoForge1\n",
      "default_name\n"
     ]
    }
   ],
   "source": [
    "result = agent.stream_return_graph_state(\"https://huggingface.co/nilq/mistral-1L-tiny, provide me a test example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

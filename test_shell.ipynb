{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from EvoForge.Tool.tools import create_execute_shell_command_tool,create_create_file_tool\n",
    "from EvoForge.config import working_directory\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/h/399/yunhengzou/genaihackthon/temporary_work_dir/Test ConvNeXt-Tiny Model')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(working_directory)/\"Test ConvNeXt-Tiny Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yunhengzou@mariana:~/genaihackthon$ PROMPT$ PROMPT$ "
     ]
    }
   ],
   "source": [
    "shell = create_execute_shell_command_tool(working_directory=Path(working_directory)/\"Test ConvNeXt-Tiny Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "PROMPT$ "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shell.func(\"micromamba activate hf\")\n",
    "shell.func(\"python test_convnext.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_tool = create_create_file_tool(working_directory=\"/h/399/yunhengzou/genaihackthon/temporary_work_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File test_mistral_tiny.py created'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_file_tool.func(**{'filename': 'test_mistral_tiny.py', 'content': 'import torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Set up a prompt\\nprompt = \"Once upon a time, there was a little girl who\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\nwith torch.no_grad():\\n    output = model.generate(\\n        inputs[\"input_ids\"],\\n        max_length=100,\\n        num_return_sequences=1,\\n        temperature=0.7,\\n        do_sample=True,\\n    )\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\\n\\nprint(\"Prompt:\", prompt)\\nprint(\"Generated text:\", generated_text)'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

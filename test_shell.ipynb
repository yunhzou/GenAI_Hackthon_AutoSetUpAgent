{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from EvoForge.Tool.tools import create_execute_shell_command_tool,create_create_file_tool\n",
    "from EvoForge.config import working_directory\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/h/399/yunhengzou/genaihackthon/temporary_work_dir/Test ConvNeXt-Tiny Model')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(working_directory)/\"Test ConvNeXt-Tiny Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yunhengzou@mariana:~/genaihackthon$ PROMPT$ PROMPT$ "
     ]
    }
   ],
   "source": [
    "shell = create_execute_shell_command_tool(working_directory=Path(working_directory)/\"Test ConvNeXt-Tiny Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|█████████████████| 266/266 [00:00<00:00, 765kB/s]\n",
      "config.json: 100%|█████████████████████████| 69.6k/69.6k [00:00<00:00, 95.3MB/s]\n",
      "pytorch_model.bin: 100%|██████████████████████| 114M/114M [00:00<00:00, 133MB/s]\n",
      "model.safetensors: 100%|█████████████████████| 114M/114M [00:01<00:00, 93.5MB/s]\n",
      "Results saved to result.json\n",
      "(hf) PROMPT$ "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'preprocessor_config.json:   0%|                       | 0.00/266 [00:00<?, ?B/s]\\rpreprocessor_config.json: 100%|█████████████████| 266/266 [00:00<00:00, 765kB/s]\\r\\n\\rconfig.json:   0%|                                  | 0.00/69.6k [00:00<?, ?B/s]\\rconfig.json: 100%|█████████████████████████| 69.6k/69.6k [00:00<00:00, 95.3MB/s]\\r\\n\\rpytorch_model.bin:   0%|                             | 0.00/114M [00:00<?, ?B/s]\\rpytorch_model.bin:   9%|█▊                  | 10.5M/114M [00:00<00:02, 51.7MB/s]\\rpytorch_model.bin:  27%|█████▍              | 31.5M/114M [00:00<00:00, 93.4MB/s]\\rpytorch_model.bin:  46%|█████████▌           | 52.4M/114M [00:00<00:00, 130MB/s]\\rpytorch_model.bin:  64%|█████████████▍       | 73.4M/114M [00:00<00:00, 154MB/s]\\rpytorch_model.bin:  82%|█████████████████▎   | 94.4M/114M [00:00<00:00, 170MB/s]\\rpytorch_model.bin: 100%|██████████████████████| 114M/114M [00:00<00:00, 173MB/s]\\rpytorch_model.bin: 100%|██████████████████████| 114M/114M [00:00<00:00, 133MB/s]\\r\\n\\rmodel.safetensors:   0%|                             | 0.00/114M [00:00<?, ?B/s]\\rmodel.safetensors:   9%|█▊                  | 10.5M/114M [00:00<00:02, 36.5MB/s]\\rmodel.safetensors:  18%|███▋                | 21.0M/114M [00:00<00:02, 36.5MB/s]\\rmodel.safetensors:  55%|███████████▌         | 62.9M/114M [00:00<00:00, 100MB/s]\\rmodel.safetensors:  73%|██████████████▋     | 83.9M/114M [00:01<00:00, 94.9MB/s]\\rmodel.safetensors:  92%|████████████████████▏ | 105M/114M [00:01<00:00, 103MB/s]\\rmodel.safetensors: 100%|█████████████████████| 114M/114M [00:01<00:00, 93.5MB/s]\\r\\nResults saved to result.json\\r\\n(hf)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shell.func(\"micromamba activate hf\")\n",
    "shell.func(\"python test_convnext.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_tool = create_create_file_tool(working_directory=\"/h/399/yunhengzou/genaihackthon/temporary_work_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File test_mistral_tiny.py created'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_file_tool.func(**{'filename': 'test_mistral_tiny.py', 'content': 'import torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel_name = \"nilq/mistral-1L-tiny\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\n# Set up a prompt\\nprompt = \"Once upon a time, there was a little girl who\"\\n\\n# Tokenize the prompt\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n# Generate text\\nwith torch.no_grad():\\n    output = model.generate(\\n        inputs[\"input_ids\"],\\n        max_length=100,\\n        num_return_sequences=1,\\n        temperature=0.7,\\n        do_sample=True,\\n    )\\n\\n# Decode the generated text\\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\\n\\nprint(\"Prompt:\", prompt)\\nprint(\"Generated text:\", generated_text)'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
